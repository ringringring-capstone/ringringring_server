{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "eb9835b0-7728-420f-bf56-0c962d774410",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer\n",
    "\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from transformers import GPT2LMHeadModel\n",
    "import huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ae289480-7763-45aa-b914-51067d90dbf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to C:\\Users\\WeSeongGu\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available())\n",
    "huggingface_hub.login(token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "32667d62-5745-43b5-882c-c9fc7adabba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': '<s>[INST]손님:효자2동 김우빈 입니다.[/INST]직원:확인 한 번 부탁드립니다. 짜장면에 효자2동이고, 김우빈 맞으실까요?</s>'}\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "# Hugging Face Basic Model 한국어 모델\n",
    "\n",
    "\n",
    "# Custom Dataset ★ 본인이 hugging face 내 저장한 모델경로를 설정해야함 ★\n",
    "hkcode_dataset = \"mogoi/delivery_all\"\n",
    "\n",
    "\n",
    "\n",
    "dataset = load_dataset(hkcode_dataset, split=\"train\")\n",
    "# 데이터 확인\n",
    "print( dataset[197] )\n",
    "print(torch.cuda.get_device_capability()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "72cc9ef0-156a-488b-a938-5045a12ec078",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.get_device_capability()[0] >= 8:\n",
    "    attn_implementation = \"flash_attention_2\"\n",
    "    torch_dtype = torch.bfloat16\n",
    "else:\n",
    "    attn_implementation = \"eager\"\n",
    "    torch_dtype = torch.float16\n",
    "\n",
    "# QLoRA config\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch_dtype,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "83f17fac-417c-4833-ae77-9ff99886302a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "646d0698-b241-4e93-9403-a4be87f6a291",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"skt/kogpt2-base-v2\",\n",
    "  bos_token='</s>', eos_token='</s>', unk_token='<unk>',\n",
    "  pad_token='<pad>', mask_token='<mask>')\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4e7ad339-1ecd-47e5-9854-61a1fb153317",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\WeSeongGu\\anaconda3\\Lib\\site-packages\\peft\\tuners\\lora\\layer.py:1059: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n",
      "C:\\Users\\WeSeongGu\\anaconda3\\Lib\\site-packages\\trl\\trainer\\sft_trainer.py:246: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de60e2859ede4e4abe8b56e90cd58e44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/53350 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "peft_params = LoraConfig(\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.1,\n",
    "        r=64,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "    )\n",
    "    \n",
    "training_params = TrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        num_train_epochs=40,\n",
    "        per_device_train_batch_size=8,\n",
    "        gradient_accumulation_steps=1,\n",
    "        optim=\"paged_adamw_32bit\",\n",
    "        save_steps=500,\n",
    "        logging_steps=500,\n",
    "        learning_rate=2e-4,\n",
    "        weight_decay=0.001,\n",
    "        fp16=False,\n",
    "        bf16=False,\n",
    "        max_grad_norm=0.3,\n",
    "        max_steps=-1,\n",
    "        warmup_ratio=0.03,\n",
    "        group_by_length=True,\n",
    "        lr_scheduler_type=\"constant\",\n",
    "        report_to=\"tensorboard\"\n",
    "    )\n",
    "    \n",
    "trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        train_dataset=dataset,\n",
    "        peft_config=peft_params,\n",
    "        dataset_text_field=\"text\",\n",
    "        max_seq_length=None,\n",
    "        tokenizer=tokenizer,\n",
    "        args=training_params,\n",
    "        packing=False,\n",
    "    )\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6ecd6a-e25e-424b-8cd2-f7b76300304d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3478, 'grad_norm': 1.100317120552063, 'learning_rate': 0.0002, 'epoch': 0.0749737591842855}\n",
      "{'loss': 0.6249, 'grad_norm': 1.057011604309082, 'learning_rate': 0.0002, 'epoch': 0.149947518368571}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\WeSeongGu\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4864, 'grad_norm': 0.8052161335945129, 'learning_rate': 0.0002, 'epoch': 0.2249212775528565}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\WeSeongGu\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3944, 'grad_norm': 0.8334420919418335, 'learning_rate': 0.0002, 'epoch': 0.299895036737142}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\WeSeongGu\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3513, 'grad_norm': 0.838429868221283, 'learning_rate': 0.0002, 'epoch': 0.3748687959214275}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\WeSeongGu\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.318, 'grad_norm': 0.685339093208313, 'learning_rate': 0.0002, 'epoch': 0.449842555105713}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\WeSeongGu\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3014, 'grad_norm': 0.6162809729576111, 'learning_rate': 0.0002, 'epoch': 0.5248163142899985}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\WeSeongGu\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2875, 'grad_norm': 0.6571316123008728, 'learning_rate': 0.0002, 'epoch': 0.599790073474284}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\WeSeongGu\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2833, 'grad_norm': 0.5563750863075256, 'learning_rate': 0.0002, 'epoch': 0.6747638326585695}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\WeSeongGu\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2697, 'grad_norm': 0.5251194834709167, 'learning_rate': 0.0002, 'epoch': 0.749737591842855}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\WeSeongGu\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2695, 'grad_norm': 0.5471196174621582, 'learning_rate': 0.0002, 'epoch': 0.8247113510271405}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\WeSeongGu\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2632, 'grad_norm': 0.593270480632782, 'learning_rate': 0.0002, 'epoch': 0.899685110211426}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\WeSeongGu\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2597, 'grad_norm': 0.5026218891143799, 'learning_rate': 0.0002, 'epoch': 0.9746588693957114}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\WeSeongGu\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2572, 'grad_norm': 0.3417667746543884, 'learning_rate': 0.0002, 'epoch': 1.049632628579997}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\WeSeongGu\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "trainer.train()\n",
    "logging.set_verbosity(logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac091a59-7aa7-42ad-bccf-596a247974ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"짜장면 배달 가능한가요?\"\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\n",
    "result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "print(result[0]['generated_text'])\n",
    "\n",
    "input_text = \"짜장면 배달 주문 가능한가요?\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "\n",
    "output = model.generate(input_ids, max_length=100, num_return_sequences=1, temperature=1.0, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(decoded_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90493089-1f5d-4787-8079-f6f15a2f71c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.push_to_hub(\n",
    "            \"fkskdldh/test-5-25\",\n",
    "            use_temp_dir=True,\n",
    "            use_auth_token=\"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\"\n",
    ")\n",
    "tokenizer.push_to_hub(\n",
    "            \"fkskdldh/test-5-25\",\n",
    "            use_temp_dir=True,\n",
    "            use_auth_token=\"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\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a32bd0-1381-4d40-844d-28fcfad4da03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_path = \"./trained_model/\"\n",
    "# tokenizer.save_pretrained(save_path)\n",
    "# torch.save(model.state_dict(), \"./trained_model/pytorch_model.bin\")\n",
    "# model.save_pretrained('./trained_model')\n",
    "# model.save_pretrained(save_path)\n",
    "# tokenizer.save_pretrained(save_path)\n",
    "# torch.save(model, save_path)\n",
    "# torch.save(tokenizer, save_path)\n",
    "tokenizer.save_pretrained('model/tokenizer/')\n",
    "torch.save(model.state_dict(), \"model/pytorch_model.bin\")\n",
    "model.save_pretrained('model/model/')\n",
    "print('finish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ac8d8f-5561-4b53-8283-56d16126e07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "save_path = \"./model/model\"\n",
    "\n",
    "loaded_model = GPT2LMHeadModel.from_pretrained(\"fkskdldh/test-5-25\")\n",
    "\n",
    "# tokenizer = PreTrainedTokenizerFast.from_pretrained(save_path)\n",
    "loaded_tokenizer = PreTrainedTokenizerFast.from_pretrained(\"fkskdldh/test-5-25\")\n",
    "input_text = \"짜장면 배달 가능한가요?\"\n",
    "input_ids = loaded_tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "\n",
    "output = loaded_model.generate(input_ids, max_length=100, num_return_sequences=1, temperature=1.0, pad_token_id=loaded_tokenizer.eos_token_id)\n",
    "\n",
    "decoded_output = loaded_tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(decoded_output)\n",
    "\n",
    "\n",
    "# input_text = \"[직원][1] 안녀하세요 락궁입니다. [손님][2]10월 20일 7시에 예약 가능한가요? [직원][3] 잠시 기달려 주세요 네 가능합니다. 네 가능합니다. 인원을 말해주세요[손님][4] 3명입니다.\"\n",
    "# input_text = \"혹시 이번주 월요일에 예약 가능한가요?\"\n",
    "# pipe = pipeline(task=\"text-generation\", model=loaded_model, tokenizer=tokenizer, max_length=200)\n",
    "# result = pipe(f\"<s>[INST] {input_text} [/INST]\")\n",
    "# print(result[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b60fac8-d2e2-4c27-a3ee-fddae4d25825",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"짜장면 배달 가능한가요?\"\n",
    "input_ids = loaded_tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "\n",
    "output = loaded_model.generate(input_ids, max_length=100, num_return_sequences=1, temperature=1.0, pad_token_id=loaded_tokenizer.eos_token_id)\n",
    "\n",
    "decoded_output = loaded_tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(decoded_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d0b8ee-8f13-41f3-96f3-ecee186ea245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델의 config 비교\n",
    "assert model.config.to_dict() == loaded_model.config.to_dict(), \"Model configs do not match!\"\n",
    "\n",
    "# 토크나이저의 config 비교\n",
    "assert tokenizer.get_vocab() == loaded_tokenizer.get_vocab(), \"Tokenizer vocabs do not match!\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
